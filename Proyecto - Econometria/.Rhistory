population<-
lapply(1:pop_size, get_chromosome, train=train, label="medv")
fitness <- lapply(population,
get_fitness,
train=train,test=test, label="medv", grado_modelo=2)
roullete<-
tibble(parent=1:pop_size, fitness= fitness %>% unlist()) %>%
arrange(desc(fitness))
roullete$rank <- 1:nrow(roullete)
roullete <-
roullete %>%
mutate(cumsum_rank = cumsum(rank))
mating_parents<-
lapply(1:100, select_mating_parents,
pop_size=pop_size,
roullete=roullete,
population=population)
children<-
lapply(mating_parents, crossover)
children<-
children %>% unlist(recursive = F)
children<-
children[1:pop_size]
new_population<-
lapply(children, mutation, rate=0.10)
fitness <- lapply(new_population,
get_fitness,
train=train,test=test, label="medv", grado_modelo=2)
roullete< -
tibble(parent=1:pop_size, fitness= fitness %>% unlist()) %>%
arrange(fitness)
roullete$rank <- 1:nrow(roullete)
roullete <-
roullete %>%
mutate(cumsum_rank = cumsum(rank))
# Finalmente, mostramos el modelo de grado 2 que mejor se acompla a los datos
top1Index <- as.integer(roullete[1,1])
print("El mejor modelo de grado 2 que mejor se acopla a los datos es:")
mse<-lapply(new_population[top1Index],get_fitness, train=train, test=test, label="medv", grado_modelo=2)
print("Error medio cuadrado:")
print(mse[[1]])
### Probamos el modelo para grado = 3
pop_size <- 100
population<-
lapply(1:pop_size, get_chromosome, train=train, label="medv")
fitness <- lapply(population,
get_fitness,
train=train,test=test, label="medv", grado_modelo=3)
roullete<-
tibble(parent=1:pop_size, fitness= fitness %>% unlist()) %>%
arrange(desc(fitness))
roullete$rank <- 1:nrow(roullete)
roullete <-
roullete %>%
mutate(cumsum_rank = cumsum(rank))
mating_parents<-
lapply(1:100, select_mating_parents,
pop_size=pop_size,
roullete=roullete,
population=population)
children<-
lapply(mating_parents, crossover)
children<-
children %>% unlist(recursive = F)
children<-
children[1:pop_size]
new_population<-
lapply(children, mutation, rate=0.10)
fitness <- lapply(new_population,
get_fitness,
train=train,test=test, label="medv", grado_modelo=3)
roullete< -
tibble(parent=1:pop_size, fitness= fitness %>% unlist()) %>%
arrange(fitness)
roullete$rank <- 1:nrow(roullete)
roullete <-
roullete %>%
mutate(cumsum_rank = cumsum(rank))
# Finalmente, mostramos el modelo de grado 3 que mejor se acompla a los datos
top1Index <- as.integer(roullete[1,1])
print("El mejor modelo de grado 3 que mejor se acopla a los datos es:")
mse<-lapply(new_population[top1Index],get_fitness, train=train, test=test, label="medv", grado_modelo=3)
print("Error medio cuadrado:")
print(mse[[1]])
library (MASS)
library (dplyr)
# Función que contiene el algoritmo de forward_stepwise_selection
forward_stepwise_selection <- function(dataset, yname) {
# Obtenemos los nombres de las features
col_names <- names(dataset)
features_disponibles <- setdiff(col_names,yname)
features_seleccionadas <- c()
p <- length(features_disponibles)
lista_se_global <- c()
while (length(features_disponibles) > 0) {
n_features_disponibles <- length(features_disponibles)
lista_se_iteracion <- c()
for (i in 1:(n_features_disponibles)) {
features_iteracion <- c(features_seleccionadas, features_disponibles[i])
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_iteracion, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el error cuadrado del modelo a nuestra lista de errores cuadrados
lista_se_iteracion <- c(lista_se_iteracion, se)
}
indice_r2_maximo_iteracion <- which.max(lista_se_iteracion)
features_seleccionadas <- c(features_seleccionadas, features_disponibles[indice_r2_maximo_iteracion])
features_disponibles <- setdiff(features_disponibles, features_disponibles[indice_r2_maximo_iteracion])
lista_se_global <- c(lista_se_global, lista_se_iteracion[indice_r2_maximo_iteracion])
print('Features: ')
print(features_seleccionadas)
print('R2: ')
print(lista_se_iteracion[indice_r2_maximo_iteracion])
}
indice_r2_maximo_global <- which.max(lista_se_global)
return (features_seleccionadas[1:indice_r2_maximo_global])
}
# Función que contiene el algoritmo de backward_stepwise_selection
backward_stepwise_selection <- function(dataset, yname) {
# Obtenemos los nombres de las features
col_names <- names(dataset)
features_seleccionadas <- setdiff(col_names,yname)
n_features_seleccionadas = length(features_seleccionadas)
lista_se_global <- c()
modelos <- list()
# Iniciamos obteniendo la información cuando se utilizan todos los features
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_seleccionadas, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el modelo que incluye todas las features así como su R2
lista_se_global <- c(lista_se_global, se)
modelos[[length(lista_se_global)]] <- features_seleccionadas
n_features_seleccionadas = length(features_seleccionadas)
print('Features: ')
print(features_seleccionadas)
print('R2: ')
print(se)
# Creamos un ciclo a partir del cual vamos eliminando un feature a la vez del modelo
while(n_features_seleccionadas > 1) {
lista_se_iteracion <- c()
for (j in 1:n_features_seleccionadas) {
features_iteracion <- setdiff(features_seleccionadas, features_seleccionadas[j])
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_iteracion, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el error cuadrado del modelo a nuestra lista de errores cuadrados
lista_se_iteracion <- c(lista_se_iteracion, se)
}
indice_r2_maximo_iteracion <- which.max(lista_se_iteracion)
features_seleccionadas <- setdiff(features_seleccionadas, features_seleccionadas[indice_r2_maximo_iteracion])
lista_se_global <- c(lista_se_global, lista_se_iteracion[indice_r2_maximo_iteracion])
modelos[[length(lista_se_global)]] <- features_seleccionadas
n_features_seleccionadas = length(features_seleccionadas)
print('Features: ')
print(features_seleccionadas)
print('R2: ')
print(lista_se_iteracion[indice_r2_maximo_iteracion])
}
indice_r2_maximo_global <- which.max(lista_se_global)
return(modelos[[indice_r2_maximo_global]])
}
# Definimos el dataset o la fuente de datos que utilizaremos
dataset<-Boston
head(dataset)
# Obtenemos el numero de features que tiene el dataset
n_features<-ncol(Boston) - 1
n_features
# Definimos el nombre del feature, dentro del dataset, que tomaremos como Y
yname <- 'medv'
# Ejecutamos la función de forward_stepwise_selection
print('FORWARD STEPWISE SELECTION')
modelo_forward <- forward_stepwise_selection(dataset, yname)
print(modelo_forward)
# Ejecutamos la función de backward_stepwise_selection
print('BAKWARD STEPWISE SELECTION')
modelo_backward <- backward_stepwise_selection(dataset, yname)
print(modelo_backward)
library (MASS)
library (dplyr)
# Función que contiene el algoritmo de forward_stepwise_selection
forward_stepwise_selection <- function(dataset, yname) {
# Obtenemos los nombres de las features
col_names <- names(dataset)
features_disponibles <- setdiff(col_names,yname)
features_seleccionadas <- c()
p <- length(features_disponibles)
lista_se_global <- c()
while (length(features_disponibles) > 0) {
n_features_disponibles <- length(features_disponibles)
lista_se_iteracion <- c()
for (i in 1:(n_features_disponibles)) {
features_iteracion <- c(features_seleccionadas, features_disponibles[i])
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_iteracion, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el error cuadrado del modelo a nuestra lista de errores cuadrados
lista_se_iteracion <- c(lista_se_iteracion, se)
}
indice_r2_maximo_iteracion <- which.max(lista_se_iteracion)
features_seleccionadas <- c(features_seleccionadas, features_disponibles[indice_r2_maximo_iteracion])
features_disponibles <- setdiff(features_disponibles, features_disponibles[indice_r2_maximo_iteracion])
lista_se_global <- c(lista_se_global, lista_se_iteracion[indice_r2_maximo_iteracion])
print('Features: ')
print(features_seleccionadas)
print('R2: ')
print(lista_se_iteracion[indice_r2_maximo_iteracion])
}
indice_r2_maximo_global <- which.max(lista_se_global)
return (features_seleccionadas[1:indice_r2_maximo_global])
}
# Función que contiene el algoritmo de backward_stepwise_selection
backward_stepwise_selection <- function(dataset, yname) {
# Obtenemos los nombres de las features
col_names <- names(dataset)
features_seleccionadas <- setdiff(col_names,yname)
n_features_seleccionadas = length(features_seleccionadas)
lista_se_global <- c()
modelos <- list()
# Iniciamos obteniendo la información cuando se utilizan todos los features
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_seleccionadas, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el modelo que incluye todas las features así como su R2
lista_se_global <- c(lista_se_global, se)
modelos[[length(lista_se_global)]] <- features_seleccionadas
n_features_seleccionadas = length(features_seleccionadas)
print('Features: ')
print(features_seleccionadas)
print('R2 maximo: ')
print(se)
# Creamos un ciclo a partir del cual vamos eliminando un feature a la vez del modelo
while(n_features_seleccionadas > 1) {
lista_se_iteracion <- c()
for (j in 1:n_features_seleccionadas) {
features_iteracion <- setdiff(features_seleccionadas, features_seleccionadas[j])
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_iteracion, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el error cuadrado del modelo a nuestra lista de errores cuadrados
lista_se_iteracion <- c(lista_se_iteracion, se)
}
indice_r2_maximo_iteracion <- which.max(lista_se_iteracion)
features_seleccionadas <- setdiff(features_seleccionadas, features_seleccionadas[indice_r2_maximo_iteracion])
lista_se_global <- c(lista_se_global, lista_se_iteracion[indice_r2_maximo_iteracion])
modelos[[length(lista_se_global)]] <- features_seleccionadas
n_features_seleccionadas = length(features_seleccionadas)
print('Features: ')
print(features_seleccionadas)
print('R2 maximo: ')
print(lista_se_iteracion[indice_r2_maximo_iteracion])
}
indice_r2_maximo_global <- which.max(lista_se_global)
return(modelos[[indice_r2_maximo_global]])
}
# Ejecución del proyecto
test <- read_csv("test.csv")
library (MASS)
library (dplyr)
install.packages("readr")
library (readr)
# Ejecución del proyecto
test <- read_csv("test.csv")
train <- read_csv("train.csv")
names(test)
names(train)
submission <- test %>% select(id)
submission$`Chance of Admit`<- runif(nrow(test))
write_csv(submission, "sub1.csv")
lm_fit <- lm(`Chance of Admit`~.,data=train)
pred_lm <- predict(lm_fit,test)
submission <- test %>% select(id)
submission$`Chance of Admit`<- pred_lm
write_csv(submission, "sub2.csv")
# Ejecución del proyecto
test <- read_csv("test.csv")
# Ejecución del proyecto
test <- read_csv("test.csv")
# Ejecución del proyecto
setwd("C:\Proyectos\Maestría en Data Science\Econometría\Proyecto")
# Ejecución del proyecto
setwd("C:\Proyectos\Maestría en Data Science\Econometría\Proyecto")
test <- read_csv("test.csv")
# Ejecución del proyecto
setwd("C:/Proyectos/Maestría en Data Science/Econometría/Proyecto")
test <- read_csv("test.csv")
train <- read_csv("train.csv")
names(test)
names(train)
submission <- test %>% select(id)
submission$`Chance of Admit`<- runif(nrow(test))
write_csv(submission, "sub1.csv")
lm_fit <- lm(`Chance of Admit`~.,data=train)
pred_lm <- predict(lm_fit,test)
submission <- test %>% select(id)
submission$`Chance of Admit`<- pred_lm
write_csv(submission, "sub2.csv")
library (MASS)
library (dplyr)
# Función que contiene el algoritmo de forward_stepwise_selection
forward_stepwise_selection <- function(dataset, yname) {
# Obtenemos los nombres de las features
col_names <- names(dataset)
features_disponibles <- setdiff(col_names,yname)
features_seleccionadas <- c()
p <- length(features_disponibles)
lista_se_global <- c()
while (length(features_disponibles) > 0) {
n_features_disponibles <- length(features_disponibles)
lista_se_iteracion <- c()
for (i in 1:(n_features_disponibles)) {
features_iteracion <- c(features_seleccionadas, features_disponibles[i])
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_iteracion, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el error cuadrado del modelo a nuestra lista de errores cuadrados
lista_se_iteracion <- c(lista_se_iteracion, se)
}
indice_r2_maximo_iteracion <- which.max(lista_se_iteracion)
features_seleccionadas <- c(features_seleccionadas, features_disponibles[indice_r2_maximo_iteracion])
features_disponibles <- setdiff(features_disponibles, features_disponibles[indice_r2_maximo_iteracion])
lista_se_global <- c(lista_se_global, lista_se_iteracion[indice_r2_maximo_iteracion])
print('Features: ')
print(features_seleccionadas)
print('R2: ')
print(lista_se_iteracion[indice_r2_maximo_iteracion])
}
indice_r2_maximo_global <- which.max(lista_se_global)
return (features_seleccionadas[1:indice_r2_maximo_global])
}
_
# Función que contiene el algoritmo de forward_stepwise_selection
forward_stepwise_selection <- function(dataset, yname) {
# Obtenemos los nombres de las features
col_names <- names(dataset)
features_disponibles <- setdiff(col_names,yname)
features_seleccionadas <- c()
p <- length(features_disponibles)
lista_se_global <- c()
while (length(features_disponibles) > 0) {
n_features_disponibles <- length(features_disponibles)
lista_se_iteracion <- c()
for (i in 1:(n_features_disponibles)) {
features_iteracion <- c(features_seleccionadas, features_disponibles[i])
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_iteracion, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el error cuadrado del modelo a nuestra lista de errores cuadrados
lista_se_iteracion <- c(lista_se_iteracion, se)
}
indice_r2_maximo_iteracion <- which.max(lista_se_iteracion)
features_seleccionadas <- c(features_seleccionadas, features_disponibles[indice_r2_maximo_iteracion])
features_disponibles <- setdiff(features_disponibles, features_disponibles[indice_r2_maximo_iteracion])
lista_se_global <- c(lista_se_global, lista_se_iteracion[indice_r2_maximo_iteracion])
print('Features: ')
print(features_seleccionadas)
print('R2: ')
print(lista_se_iteracion[indice_r2_maximo_iteracion])
}
indice_r2_maximo_global <- which.max(lista_se_global)
return (features_seleccionadas[1:indice_r2_maximo_global])
}
# Función que contiene el algoritmo de backward_stepwise_selection
backward_stepwise_selection <- function(dataset, yname) {
# Obtenemos los nombres de las features
col_names <- names(dataset)
features_seleccionadas <- setdiff(col_names,yname)
n_features_seleccionadas = length(features_seleccionadas)
lista_se_global <- c()
modelos <- list()
# Iniciamos obteniendo la información cuando se utilizan todos los features
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_seleccionadas, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el modelo que incluye todas las features así como su R2
lista_se_global <- c(lista_se_global, se)
modelos[[length(lista_se_global)]] <- features_seleccionadas
n_features_seleccionadas = length(features_seleccionadas)
print('Features: ')
print(features_seleccionadas)
print('R2 maximo: ')
print(se)
# Creamos un ciclo a partir del cual vamos eliminando un feature a la vez del modelo
while(n_features_seleccionadas > 1) {
lista_se_iteracion <- c()
for (j in 1:n_features_seleccionadas) {
features_iteracion <- setdiff(features_seleccionadas, features_seleccionadas[j])
# Creamos la formula para los features de la iteracion
formula_x <- paste0(features_iteracion, collapse = "+")
formula_x <- paste0(yname, "~", formula_x, collapse = "")
# Evaluamos el modelo a partir de la formula
fit <- lm(formula=formula_x, data=dataset)
#test_perd <- predict(fit, dataset)
# Calculamos el error cuadrado
#se <- sum((test_perd - dataset[[yname]])^2)
se <- summary(fit)$r.squared
# Agregamos el error cuadrado del modelo a nuestra lista de errores cuadrados
lista_se_iteracion <- c(lista_se_iteracion, se)
}
indice_r2_maximo_iteracion <- which.max(lista_se_iteracion)
features_seleccionadas <- setdiff(features_seleccionadas, features_seleccionadas[indice_r2_maximo_iteracion])
lista_se_global <- c(lista_se_global, lista_se_iteracion[indice_r2_maximo_iteracion])
modelos[[length(lista_se_global)]] <- features_seleccionadas
n_features_seleccionadas = length(features_seleccionadas)
print('Features: ')
print(features_seleccionadas)
print('R2 maximo: ')
print(lista_se_iteracion[indice_r2_maximo_iteracion])
}
indice_r2_maximo_global <- which.max(lista_se_global)
return(modelos[[indice_r2_maximo_global]])
}
# Definimos el dataset o la fuente de datos que utilizaremos
dataset<-Boston
head(dataset)
# Obtenemos el numero de features que tiene el dataset
n_features<-ncol(Boston) - 1
n_features
# Definimos el nombre del feature, dentro del dataset, que tomaremos como Y
yname <- 'medv'
# Ejecutamos la función de forward_stepwise_selection
print('FORWARD STEPWISE SELECTION')
modelo_forward <- forward_stepwise_selection(dataset, yname)
print(modelo_forward)
# Ejecución del proyecto
setwd("C:/Proyectos/Maestría en Data Science/Econometría/Proyecto")
test <- read_csv("test.csv")
train <- read_csv("train.csv")
names(test)
names(train)
submission <- test %>% select(id)
print(submission)
names(train)
names(test)
# Ejecución del proyecto
setwd("C:/Proyectos/Maestría en Data Science/Econometría/Proyecto")
test <- read_csv("test.csv")
train <- read_csv("train.csv")
names(test)
names(train)
yname <- 'Chance of Admit'
modelo_forward <- forward_stepwise_selection(train, yname)
yname <- `Chance of Admit`
yname <- `Chance of Admit`
names(test)
test <- read_csv("test.csv")
train <- read_csv("train.csv")
names(test)
names(train)
# Ejecución del proyecto
setwd("C:/Proyectos/Maestría en Data Science/Econometría/Proyecto")
test <- read_csv("test.csv")
train <- read_csv("train.csv")
names(test)
names(train)
submission <- test %>% select(id)
submission$`Chance of Admit`<- runif(nrow(test))
write_csv(submission, "sub1.csv")
yname <- `Chance of Admit`
